{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 크롤링할 웹사이트 URL 정의\n",
    "URL = 'https://www.jobkorea.co.kr/starter/PassAssay'\n",
    "\n",
    "# 분야 코드 리스트\n",
    "schParts = ['10026', '10027', '10028', '10029', '10030', '10031', '10032', '10033', '10034', '10035', '10036', '10037', '10038', '10039', '10040', '10041', '10042', '10043', '10044', '10045']\n",
    "\n",
    "def crawl_page(page, schPart):\n",
    "    # 요청을 위한 파라미터 설정\n",
    "    params = {\n",
    "        'schPart': schPart,\n",
    "        'Page': page\n",
    "    }\n",
    "\n",
    "    res = requests.get(URL, params=params)\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    # 각 행에서 회사, 날짜, 업무 유형, 직무, 자기소개서 링크 정보 추출\n",
    "    for company_element, date_element, work_type_element, job_element, cover_letter_link_element in zip(\n",
    "        soup.select('.titTx'),\n",
    "        soup.select('.career')[1:],\n",
    "        soup.select('.linkArray .field')[0::2],\n",
    "        soup.select('.linkArray .field')[1::2],\n",
    "        soup.select('.selfLists a')[::5],\n",
    "    ):\n",
    "        # 추출한 데이터 text로 추출(/r,/n 제거)\n",
    "        company, date, work_type, job = company_element.get_text(), date_element.get_text(), work_type_element.get_text(), job_element.get_text()\n",
    "        cover_letter_url = 'https://www.jobkorea.co.kr' + cover_letter_link_element['href']\n",
    "\n",
    "        # 자기소개서 페이지에서 질문과 답변 추출\n",
    "        cover_letter_res = requests.get(cover_letter_url)\n",
    "        cover_letter_soup = BeautifulSoup(cover_letter_res.text, 'html.parser')\n",
    "\n",
    "        question_elements = cover_letter_soup.select('.qnaLists .tx')[::2]\n",
    "        answer_elements = cover_letter_soup.select('.qnaLists .tx')[1::2]\n",
    "\n",
    "        questions = [q.get_text() for q in question_elements]\n",
    "        answers = [a.get_text() for a in answer_elements]\n",
    "        \n",
    "        # 질문/답변 1개 쌍으로 1행씩 묶기\n",
    "        for q, a in zip(questions, answers):\n",
    "            rows.append({'company': company, 'date': date, 'work_type': work_type, 'job': job, 'question': q, 'answer': a})\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "for idx, schPart in enumerate(schParts):\n",
    "    result_df = pd.DataFrame(columns=['company', 'date', 'work_type', 'job', 'question', 'answer'])\n",
    "    page = 1\n",
    "\n",
    "    # 페이지가 비어있을 때까지 크롤링을 계속\n",
    "    while True:\n",
    "        current_df = crawl_page(page, schPart)\n",
    "        if current_df is None or current_df.empty:\n",
    "            break\n",
    "        result_df = pd.concat([result_df, current_df], ignore_index=True)\n",
    "        page += 1\n",
    "\n",
    "    # 파일 이름 지정 및 데이터프레임 저장\n",
    "    filename = f'jobkorea_data{idx}.csv'\n",
    "    result_df.to_csv(filename, index=False, encoding='utf-8-sig')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
