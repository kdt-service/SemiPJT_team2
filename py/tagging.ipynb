{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "df = pd.read_csv('./jobkorea_unique0.95.csv')\n",
    "\n",
    "# 한국어 불용어 불러오기\n",
    "# 공백문자 제거하고 set으로 저장함(중복제거)\n",
    "with open('stopwords-ko.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = set(line.strip() for line in f)\n",
    "\n",
    "# 텍스트를 입력받아 불용어 처리 후 Mecab을 사용하여 품사 태깅을 수행하는 함수\n",
    "def mecab_tokenize(text, stopwords):\n",
    "    Tag = []\n",
    "    mecab = Mecab(dicpath=r\"C:/mecab/mecab-ko-dic\")\n",
    "    tokens = mecab.morphs(text)\n",
    "    # 불용어가 아닌 단어들을 태깅\n",
    "    filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    for token in filtered_tokens:\n",
    "        tagged_token = mecab.pos(token)\n",
    "        # 'NNG', 'NNP', 'NNB', 'NR', 'VV' 태그를 가진 토큰만 추가\n",
    "        # ('단어','태그')형태이므로 태그부분 참조\n",
    "        Tag += [t for t in tagged_token if t[1] in ('NNG', 'NNP', 'NNB', 'NR', 'VV')]\n",
    "    return Tag  # 태깅된 토큰들\n",
    "\n",
    "# 데이터프레임에 불용어 처리된 품사 태깅 결과를 저장\n",
    "df['pos'] = df['answer'].apply(lambda x: mecab_tokenize(x, stopwords))\n",
    "df.to_csv('jobkorea_pos_mecab_NV_0.95.csv', index=False, encoding='UTF-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "df = pd.read_csv('./jobkorea_unique0.95.csv')\n",
    "\n",
    "# 한국어 불용어 불러오기\n",
    "# 공백문자 제거하고 set으로 저장함(중복제거)\n",
    "with open('stopwords-ko.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = set(line.strip() for line in f)\n",
    "\n",
    "# 텍스트를 입력받아 불용어 처리 후 Okt를 사용하여 품사 태깅을 수행하는 함수\n",
    "def okt_tokenize(text, stopwords):\n",
    "    Tag = []\n",
    "    okt = Okt()\n",
    "    tokens = okt.morphs(text)\n",
    "    # 불용어가 아닌 단어들을 태깅\n",
    "    filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    for token in filtered_tokens:\n",
    "        tagged_token = okt.pos(token)\n",
    "        # 'Noun', 'Verb' 태그를 가진 토큰만 추가\n",
    "        # ('단어','태그')형태이므로 태그부분 참조\n",
    "        Tag += [t for t in tagged_token if t[1] in ('Noun', 'Verb')]\n",
    "    return Tag  # 태깅된 토큰들\n",
    "\n",
    "# 데이터프레임에 불용어 처리된 품사 태깅 결과를 저장\n",
    "df['pos'] = df['answer'].apply(lambda x: okt_tokenize(x, stopwords))\n",
    "df.to_csv('jobkorea_pos_okt_NV_0.95.csv', index=False, encoding='UTF-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
